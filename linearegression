1.Linear Regression table provides a comprehensive overview of your model. It shows you everything about how well your line fits your data points. This table is typically generated by statsmodels' OLS (Ordinary Least Squares) model.

* Example: Like a report card for the model, showing grades (coefficients), pass/fail (p-values), and overall performance (R-squared).  

2.Regression Info. These are the basic facts about your model. Such as:

- R-squared: How much of the change in the output is explained by the inputs.  
- F-statistic: Tells if the model is better than just guessing the average.  
- P-value (for the model): Whether the model is useful overall (if < 0.05, it is).  

3.Regression Coefficients. Coefficients tell the relationship between input and output.

These are the "weights" or "importance" of each input variable in predicting the output.  
- A positive coefficient means increasing that input increases the output.  
- A negative coefficient means increasing that input decreases the output.  

4.Regression P-Value. P-Values help determine statistical significance

- P-value < 0.05: The variable is important (we keep it).  
- P-value > 0.05: The variable may not matter (we might remove it).  

5.Regression R-Squared tells how well the line explains the data.

This measures how well the model fits the data (0% to 100%).  
- 0%: The model explains none of the changes in the output.  
- 100%: The model perfectly explains the output (rare in real life).
1.Linear Regression table provides a comprehensive overview of your model. It shows you everything about how well your line fits your data points. This table is typically generated by statsmodels' OLS (Ordinary Least Squares) model.

* Example: Like a report card for the model, showing grades (coefficients), pass/fail (p-values), and overall performance (R-squared).  

2.Regression Info. These are the basic facts about your model. Such as:

- R-squared: How much of the change in the output is explained by the inputs.  
- F-statistic: Tells if the model is better than just guessing the average.  
- P-value (for the model): Whether the model is useful overall (if < 0.05, it is).  

3.Regression Coefficients. Coefficients tell the relationship between input and output.

These are the "weights" or "importance" of each input variable in predicting the output.  
- A positive coefficient means increasing that input increases the output.  
- A negative coefficient means increasing that input decreases the output.  

4.Regression P-Value. P-Values help determine statistical significance

- P-value < 0.05: The variable is important (we keep it).  
- P-value > 0.05: The variable may not matter (we might remove it).  

5.Regression R-Squared tells how well the line explains the data.

This measures how well the model fits the data (0% to 100%).  
- 0%: The model explains none of the changes in the output.  
- 100%: The model perfectly explains the output (rare in real life).

import numpy as np
import pandas as pd
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(42)
X = np.random.rand(50) * 10            
y = 3 * X + np.random.randn(50) * 2   

# Create a DataFrame
df = pd.DataFrame({'X': X, 'y': y})

# Add constant (for intercept term)
X_with_const = sm.add_constant(df['X'])

# Fit the linear regression model
model = sm.OLS(df['y'], X_with_const).fit()

# Print the regression summary
print(model.summary())

# Intercept and slope
print("Coefficients:")
print(model.params)

print("P-values:")
print(model.pvalues)

print("R-squared:", model.rsquared)
